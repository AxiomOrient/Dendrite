# Dendrite 라이브러리 설계 및 구현 철학 (INFO.md)

**버전: 2.0**

이 문서는 `Dendrite` 라이브러리의 핵심 설계 원칙, 아키텍처, 그리고 구현 뒤에 숨겨진 기술적 결정들을 기록합니다.

---

### 1. 핵심 철학: "디지털 고고학자 (Digital Archaeologist)"

`Dendrite`의 가장 근본적인 철학은 **원본의 의도를 보존하는 것**입니다. 우리는 단순한 텍스트 파서를 만드는 것이 아니라, 문서의 구조, 계층, 그리고 각 요소 간의 관계를 원본 그대로 발굴하고 보존하는 '디지털 고고학자'의 역할을 수행하고자 합니다.

모든 후속 처리(청킹, 임베딩)는 `Dendrite`가 발굴한 이 **"의미 지도(Semantic Map)"** 위에서 이루어집니다. 이는 최종적으로 RAG 시스템이 "왜 이 답변을 했는가?"라는 질문에 대해 명확한 근거를 제시하고, 사용자가 그 근거를 신뢰하며 원본 데이터까지 탐색할 수 있게 하는 기반이 됩니다.

### 2. 아키텍처 원칙 및 적용된 디자인 패턴

이러한 철학을 구현하기 위해, 우리는 다음과 같은 아키텍처 원칙과 디자인 패턴을 채택했습니다.

#### 가. 프로토콜 지향 프로그래밍 (Protocol-Oriented Programming)
- **구현**: `ParserProtocol`이 대표적인 예입니다.
- **이유**: 새로운 파일 형식(예: `DOCX`, `XML`)을 지원해야 할 때, 라이브러리의 핵심 코드를 수정할 필요 없이 `ParserProtocol`을 준수하는 새로운 파서만 추가하면 됩니다. 이는 **개방-폐쇄 원칙(OCP)**을 따르며, 시스템의 유연성과 확장성을 극대화합니다.

#### 나. 전략 패턴 (Strategy Pattern)
- **구현**: `HTMLParser`가 사용하는 `TagConverters.swift`의 변환기들이 완벽한 예시입니다.
- **이유**: 각 HTML 태그(`<h1>`, `<p>`, `<li>` 등)를 처리하는 방법을 별도의 '전략' 객체로 캡슐화했습니다. 새로운 태그에 대한 처리 규칙을 추가하고 싶을 때, 해당 태그에 대한 변환기(`TagConverting` 준수)만 새로 추가하면 되므로, 기존 코드에 영향을 주지 않고 기능을 확장할 수 있습니다.

#### 다. 동시성 안정성을 위한 액터 모델 (Actor Model)
- **구현**: `HierarchicalChunker`를 `struct`에서 `actor`로 전환했습니다.
- **이유**: 청킹 과정은 여러 상태(현재 청크에 쌓인 노드, 토큰 수, breadcrumb 스택 등)를 순차적으로 변경하며 진행됩니다. 만약 여러 문서가 동시에 처리될 경우, 이러한 공유 상태에 대한 접근은 데이터 경쟁(Data Race)을 일으킬 수 있습니다. `actor`는 내부 상태를 외부의 동시 접근으로부터 안전하게 보호하여, 여러 문서 처리 작업이 병렬로 수행되더라도 각 작업의 무결성을 보장합니다.

### 3. 데이터 처리 파이프라인: `SemanticNode`를 중심으로

`Dendrite`의 모든 데이터 흐름은 **`SemanticNode`** 라는 표준화된 중간 표현(Intermediate Representation)을 중심으로 이루어집니다.

1.  **파싱 (Parsing)**: `ParserFactory`는 입력된 파일의 `UTType`에 맞는 파서(`MarkdownParser` 등)를 선택합니다. 선택된 파서는 원본 문서를 분석하여, 그 구조와 내용을 담은 `[SemanticNode]` 트리로 변환합니다.

2.  **`SemanticNode` (의미 지도)**: 이 단계의 결과물은 단순 텍스트가 아닌, 문서의 구조를 완벽히 표현하는 트리입니다.
    - **계층 구조**: `heading`, `list`, `paragraph` 등의 관계를 통해 문서의 논리적 흐름을 보존합니다.
    - **고유 식별자(ID)**: **모든 구조적 노드(블록 레벨 요소)는 고유한 ID를 가집니다.** 이는 어떤 노드가 어떤 청크의 근원이 되었는지 명확히 추적하기 위함이며, '신뢰 기반 RAG'의 핵심인 **정확한 출처 표시(Citation)** 기능을 구현하는 데 필수적인 기술적 토대입니다.

3.  **의미론적 청킹 (Semantic Chunking)**: `HierarchicalChunker`는 `SemanticNode` 트리를 입력받아 최종적인 `[Chunk]` 배열을 생성합니다. 이 과정은 다음 3대 원칙을 따릅니다.
    - **원칙 1: 내용의 원자성(Atomicity) 존중**: 일반 문단, 테이블 행, 코드 블록은 절대 하나의 청크에 섞이지 않습니다. 각자는 고유한 의미를 가진 '원자'로 취급됩니다.
    - **원칙 2: 컨텍스트 경계(Context Boundary) 명확화**: `heading` 노드는 강력한 컨텍스트 경계입니다. 새로운 제목이 나타나면, 이전 내용은 별도의 청크로 완결되어 주제가 섞이는 것을 방지합니다.
    - **원칙 3: 소비자(LLM) 고려 포맷팅**: 청크의 최종 소비자는 LLM입니다. 테이블 행을 청크로 만들 때, 단순히 `|`로 구분된 텍스트가 아닌, `zip(headers, row)`을 통해 **"헤더: 값"** 형태의 명확한 Key-Value 쌍으로 콘텐츠를 가공합니다. 이는 LLM이 데이터의 의미를 훨씬 정확하게 이해하도록 돕습니다.

### 4. 최종 산출물: `Chunk`

이 모든 과정을 거쳐 생성된 `Chunk` 객체는 다음과 같은 풍부한 정보를 담고 있습니다.

- `id`: 청크의 고유 ID
- `content`: LLM이 이해하기 쉬운 형태로 가공된 순수 텍스트
- `breadcrumb`: 문서 내에서의 계층적 위치 ("문서 제목 > 챕터 1 > 섹션 2")
- `sourceNodes`: 이 청크가 어떤 `SemanticNode`들로부터 생성되었는지에 대한 참조
- `documentMetadata`: 원본 문서의 제목, 저자 등 전역 메타데이터

이 구조화된 `Chunk`는 후속 임베딩 및 검색 단계에서 RAG 시스템의 성능을 극대화하는 가장 중요한 자산이 됩니다.


---

최종 방향성: '신뢰 기반 탐색형(Trust-based Exploratory)' RAG 아키텍처
이 아키텍처는 단순히 질문에 답하는 것을 넘어, 사용자가 자신의 데이터를 신뢰하며 탐색하고, 새로운 인사이트를 얻도록 돕는 것에 중점을 둡니다.

1. 전처리 (Parsing & Preprocessing): 소스의 구조를 완벽하게 보존
구조적 파싱: Dendrite(가칭)를 통해 마크다운, PDF, HTML 등 다양한 문서의 구조(제목, 목록, 표, 이미지 캡션 등)를 단순 텍스트가 아닌, 계층적 메타데이터로 정확히 파싱합니다.

특수 콘텐츠 처리:

테이블: 각 행(row)을 독립된 정보 단위로 변환하되, 테이블 전체의 제목과 맥락을 메타데이터로 포함합니다.

코드: 함수/클래스 단위로 분리하고, 해당 코드의 목적을 설명하는 주석이나 주변 텍스트를 메타데이터로 함께 저장합니다.

2. 청킹 (Chunking): 의미론적 경계를 절대 기준으로
계층적 청킹: 오버랩 없이, 오직 문서의 구조(H1 → H2 → 단락)를 기준으로 의미가 완결되는 단위로만 분할합니다. 이는 NotebookLM이 답변의 근거를 명확히 제시하는 것과 직결됩니다.

청크 ID 생성: 모든 청크는 [문서 ID]_[청크 순번]과 같이 고유하고 예측 가능한 ID를 가집니다. 이는 답변 생성 후 정확한 출처(Citation)를 표기하는 데 필수적입니다.

3. 강화 및 인덱싱 (Enrichment & Indexing): 다각적 검색을 위한 다중 표현
메타데이터 강화: 모든 청크에 문서 제목, 저자, 생성일 및 계층 구조(Breadcrumb) 정보를 풍부하게 추가합니다.

다중 표현 인덱싱 (Multi-Representation Indexing):

원문(Original Text): 키워드 검색(BM25)을 위해 원문 그대로 인덱싱합니다.

요약문(Summary): LLM을 통해 각 청크에 대한 요약문을 생성하고, 이를 **벡터 검색(Vector Search)**의 대표 임베딩으로 사용합니다. 이는 넓은 의미의 질문에 더 정확하게 반응합니다.

가상 질문(Hypothetical Questions): 각 청크의 내용으로 답변할 수 있는 예상 질문들을 LLM으로 생성하고, 이 질문들을 임베딩하여 '질문으로 질문을 찾는' 방식을 구현합니다.

4. 검색 (Retrieval): 정확도와 재현율을 모두 잡는 다단계 필터링
쿼리 변환 (Query Transformation): 사용자 질문을 LLM을 통해 분석하여 검색 의도를 명확히 한 여러 개의 하위 쿼리(키워드, 자연어, 가상 질문 형태)로 확장합니다.

다중 벡터 검색 (Multi-Vector Retrieval): 확장된 쿼리를 사용해 '요약문 임베딩'과 '가상 질문 임베딩' 공간에서 각각 후보군을 검색합니다.

하이브리드 검색 (Hybrid Search): 키워드 검색(BM25) 결과를 통합합니다.

재정렬 (Re-ranking): 검색된 모든 후보군을 **재정렬 모델(Re-ranker)**을 통해 최종 질문과의 관련성이 가장 높은 순으로 정렬하여 상위 N개를 선별합니다.

5. 생성 및 상호작용 (Generation & Interaction): 신뢰와 탐색의 사용자 경험 (UX)
근거 기반 생성 (Grounded Generation):

최종 선별된 청크(Context)와 원본 질문을 LLM에 전달합니다.

프롬프트에 "주어진 컨텍스트만을 사용하여 답변하고, 각 문장의 근거가 되는 청크 ID를 [doc1_chunk_5] 형식으로 반드시 인용(Cite)하라"는 강력한 제약 조건을 부여합니다.

인용(Citation) 및 소스 하이라이팅:

LLM이 생성한 답변에서 인용된 [doc1_chunk_5]와 같은 ID를 파싱합니다.

UI에서 해당 인용 번호를 클릭하면, 원본 문서의 해당 청크 위치로 스크롤 되거나 팝업으로 보여주는 기능을 구현합니다. 이것이 NotebookLM의 핵심적인 신뢰 장치입니다.

탐색 도구 제공 (Notebook Guide):

사용자가 업로드한 문서 전체에 대해 자동으로 **'노트북 가이드'**를 생성합니다.

문서 전체 요약: 핵심 내용을 요약하여 제공합니다.

추천 질문: 문서 내용을 기반으로 탐색을 유도하는 질문 목록을 제시합니다.

자동 문서 생성: FAQ, 학습 가이드, 브리핑 자료 등 정형화된 형식의 문서를 원클릭으로 생성하는 기능을 제공합니다.

